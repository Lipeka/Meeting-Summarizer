{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lipeka/Meeting-Summarizer/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "gwtpEfVXbHN1",
        "outputId": "7c02cac9-8c41-4c84-c41c-7cac0e01d9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5293235bfa1eb4dbf2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5293235bfa1eb4dbf2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 🛠️ Install dependencies\n",
        "!pip install -q gradio openai-whisper pydub ffmpeg-python\n",
        "\n",
        "# ✅ Imports\n",
        "import gradio as gr\n",
        "import whisper\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# 🔐 OpenRouter API Key and Model\n",
        "OPENROUTER_API_KEY = \"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\"\n",
        "MODEL = \"mistralai/mistral-7b-instruct:free\"\n",
        "\n",
        "# 🔊 Transcription using Whisper\n",
        "def whisper_transcribe(audio_path):\n",
        "    try:\n",
        "        model = whisper.load_model(\"large\")  # Use \"large\" for more accuracy if needed\n",
        "        result = model.transcribe(audio_path)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Whisper error: {str(e)}\"\n",
        "\n",
        "# 🧠 Summarization using OpenRouter\n",
        "def summarize_meeting(transcript):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://your-site.com\",\n",
        "        \"X-Title\": \"Meeting Summarizer\",\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "From the following meeting transcript, extract only the task assignments.\n",
        "\n",
        "Return a JSON array where each object has:\n",
        "- assigned_to\n",
        "- department\n",
        "- task_description\n",
        "- deadline\n",
        "\n",
        "Do not include general updates, announcements, welcomes, or summaries.\n",
        "Only include actual responsibilities or assignments clearly given to individuals or teams.\n",
        "\n",
        "Meeting Transcript:\n",
        "{transcript}\n",
        "\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    r = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    if r.status_code == 200:\n",
        "        return r.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"API Error: {r.status_code} - {r.text}\"\n",
        "\n",
        "# 🔄 Main processing function\n",
        "def process_input(audio_file, manual_text):\n",
        "    if audio_file is not None:\n",
        "        # Convert MP3 to WAV if needed\n",
        "        if audio_file.endswith(\".mp3\"):\n",
        "            sound = AudioSegment.from_mp3(audio_file)\n",
        "            wav_path = audio_file.replace(\".mp3\", \".wav\")\n",
        "            sound.export(wav_path, format=\"wav\")\n",
        "            audio_file = wav_path\n",
        "\n",
        "        transcript = whisper_transcribe(audio_file)\n",
        "    elif manual_text.strip() != \"\":\n",
        "        transcript = manual_text.strip()\n",
        "    else:\n",
        "        return \"No input provided.\", \"Please upload audio or enter text.\"\n",
        "\n",
        "    summary = summarize_meeting(transcript)\n",
        "    return transcript, summary\n",
        "\n",
        "# 🎛️ Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎙️ Meeting Task Extractor\\nUpload an audio file or paste your meeting transcript to extract tasks.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(label=\"Upload Audio (.wav or .mp3)\", type=\"filepath\")\n",
        "        manual_input = gr.Textbox(label=\"Or paste transcript manually\", lines=10, placeholder=\"Paste transcript here...\")\n",
        "\n",
        "    with gr.Row():\n",
        "        transcribed_output = gr.Textbox(label=\"📝 Transcribed / Input Text\", lines=10)\n",
        "        json_output = gr.Textbox(label=\"📋 Task Summary (JSON)\", lines=10)\n",
        "\n",
        "    btn = gr.Button(\"Process\")\n",
        "    btn.click(fn=process_input, inputs=[audio_input, manual_input], outputs=[transcribed_output, json_output])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNj1qrU5yidjlFe1RIMCtSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}